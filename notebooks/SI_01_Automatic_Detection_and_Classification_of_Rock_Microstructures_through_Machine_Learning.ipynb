{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Detection and Classification of Rock Microstructures through Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "List authors, their current affiliations,  up-to-date contact information, and ORCID if available. Add as many author lines as you need. \n",
    "\n",
    "- Author1 = {\"name\": \"Stephen Iota\", \"affiliation\": \"USC\", \"email\": \"iota@usc.edu\", \"orcid\": \"\"}\n",
    "- Author2 = {\"name\": \"Junyi Liu\", \"affiliation\": \"USC\", \"email\": \"liujunyi@usc.edu\", \"orcid\": \"\"}\n",
    "- Author3 = {\"name\": \"Ming Lyu\", \"affiliation\": \"USC\", \"email\": \"minglyu@usc.edu\", \"orcid\": \"\"}\n",
    "- Author4 = {\"name\": \"Bolong Pan\", \"affiliation\": \"USC\", \"email\": \"bolongpa@usc.edu\", \"orcid\": \"\"}\n",
    "- Author5 = {\"name\": \"Xiaoyu Wang\", \"affiliation\": \"USC\", \"email\": \"xwang93@usc.edu\", \"orcid\": \"\"}\n",
    "- Author6 = {\"name\": \"Yolanda Gil\", \"affiliation\": \"USC\", \"email\": \"gil@isi.edu\", \"orcid\": \"\"}\n",
    "- Author7 = {\"name\": \"Wael AbdAlmageed\", \"affiliation\": \"USC\", \"email\": \"wamageed@isi.edu\", \"orcid\": \"\"}\n",
    "- Author8 = {\"name\": \"Gurman Gill\", \"affiliation\": \"Sonoma State\", \"email\": \"gillg@sonoma.edu\", \"orcid\": \"\"}\n",
    "- Author9 = {\"name\": \"Matty Mookerjee\", \"affiliation\": \"Sonoma State\", \"email\": \"matty.mookerjee@sonoma.edu\", \"orcid\": \"\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Template-Notebook-for-EarthCube---Long-Version\" data-toc-modified-id=\"Template-Notebook-for-EarthCube---Long-Version-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Template Notebook for EarthCube - Long Version</a></span><ul class=\"toc-item\"><li><span><a href=\"#Author(s)\" data-toc-modified-id=\"Author(s)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Author(s)</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Technical-contributions\" data-toc-modified-id=\"Technical-contributions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Technical contributions</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Methodology</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Work-In-Progress---improvements\" data-toc-modified-id=\"Work-In-Progress---improvements-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Work In Progress - improvements</a></span></li><li><span><a href=\"#Suggested-next-steps\" data-toc-modified-id=\"Suggested-next-steps-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Suggested next steps</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Acknowledgements</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-import\" data-toc-modified-id=\"Library-import-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library import</a></span></li><li><span><a href=\"#Local-library-import\" data-toc-modified-id=\"Local-library-import-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Local library import</a></span></li></ul></li><li><span><a href=\"#Parameter-definitions\" data-toc-modified-id=\"Parameter-definitions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameter definitions</a></span></li><li><span><a href=\"#Data-import\" data-toc-modified-id=\"Data-import-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data import</a></span></li><li><span><a href=\"#Data-processing-and-analysis\" data-toc-modified-id=\"Data-processing-and-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data processing and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-10-rules\" data-toc-modified-id=\"The-10-rules-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>The 10 rules</a></span></li><li><span><a href=\"#Using-notebook-template\" data-toc-modified-id=\"Using-notebook-template-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Using notebook template</a></span></li><li><span><a href=\"#Adding-table-of-contents\" data-toc-modified-id=\"Adding-table-of-contents-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Adding table of contents</a></span></li><li><span><a href=\"#Creating-Binder-and-Docker-for-your-notebook-repository\" data-toc-modified-id=\"Creating-Binder-and-Docker-for-your-notebook-repository-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Creating Binder and Docker for your notebook repository</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Geologists need help classifying microscope rock images of sigma clasts; a type of mantled porphyroclasts widely used as kinematic indicators in rocks. \n",
    "Knowledge about the sheer sense of sigma clast during formation (either CCW or CW sheering) gives insights into rock formation history.  \n",
    "\n",
    "This work reports on early investigation of machine learning techniques for automatic detection and classification of sigma clasts and their rotation from photomicrographs. \n",
    "Convolutional Neural Networks (CNNs) are used to extract and leverage defining features of sigma clasts, such as shape, color, texture, and tail direction to improve accuracy. \n",
    "\n",
    "We leverage existing models that are pre-trained on very large collections of images, and use transfer learning techniques to apply them to microstructure images.  \n",
    "\n",
    "We used YOLOv3 to identify different sigma clasts in a given image. We also experimented with other large pre-trained models such as ResNet50, VGG19, Inceptionv3 with two additional layers trained specifically on our dataset. \n",
    "\n",
    "In order to facilitate exploration of different models with different settings, we are developing a computational experimentation environment to visualize different CNN network layers, classification heatmaps, and comparative metrics. \n",
    "\n",
    "Finally, since models perform better when more data are available, we are developing a web application to collect additional data from geoscientists and incentivize their participation in open science. \n",
    "\n",
    "The website allows researchers to upload images of rock microstructures, showing them the classification of the images based on the best models available, and allows them to correct any errors which can be used to improve the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical contributions\n",
    "Summarize the central contributions of the notebook (libraries created, scientific analysis demonstrated, etc.), as a bulleted list, with one or more bullets.\n",
    "\n",
    "\n",
    "- Exploring the use of Machine Learning models for fast analysis and classification of geology data\n",
    "- Create new user website to facilitate with data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "< Quickly describe assumptions and processing steps. Include URLs as necessary. >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "< Describe and comment on the most important results. Include images and URLs as necessary. >\n",
    "\n",
    "< describe main results from results matrix here >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding\n",
    "- No funding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "keywords=[\"Sigma clast\", \"classification\", \"machine learning\", \"AI for geosciences\", \"CNNs for image classification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "Automatic Detection and Classification of Rock Microstructures through Machine Learning, EarthCube Annual Meeting, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work In Progress - improvements\n",
    "\n",
    "Notable TODOs:\n",
    "- Data augmentation;\n",
    "- Integrate image recognition models with user website;\n",
    "- Explore custom made Neural Network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested next steps\n",
    "< State suggested next steps, based on results obtained in this notebook. This section is optional. >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements \n",
    "\n",
    "This notebook template extends the original notebook template provided with the jupytemplate extension [5]. It is a result of collaboration between the TAC Working Group and the EarthCube Office. \n",
    "\n",
    "The template is licensed under a <a href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add any additional imports you needed down below, try to group them by their purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std library \n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML models\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "import keras\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50,VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Visualizations and display\n",
    "import tensorboard\n",
    "print(f\"tensorboard version: {tensorboard.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import cv2\n",
    "from IPython.display import Image,display\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "# Inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Autoreload extension\n",
    "#if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "#    %load_ext autoreload\n",
    "    \n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "Import all the required local libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## None needed so far, but add them here if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions\n",
    "< Set all relevant parameters for the notebook. By convention, parameters are uppercase, while all the other variables follow Python's guidelines. >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n",
    "Retrieve all the required data for the analysis. Include brief data descriptions, and DOIs where possible.\n",
    "\n",
    "Please import data in one of two ways:\n",
    "1. Reference data that is available online and continuously managed (preferred). \n",
    "2. If the data is relatively small, please include the files along with the notebook. If you deal with large data, include a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we should import all the data files,\n",
    "## we need to make all data available locally (inside a Binder),\n",
    "## please make sure to also upload the data set you used to the github, as the notebook \n",
    "## needs to be self contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stephens_path = Path(\"./lowres_data/\")\n",
    "files = Path(\"./original_data\")\n",
    "heatmap_files = sorted(glob(str(files)+\"/CW/*.jpg\"))\n",
    "\n",
    "# add path to data here, even if it's redundant, we'll organize later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n",
    "The core of the notebook is here. Split this section into subsections as required, and explain processing and analysis steps.\n",
    "\n",
    "Selected best practices for organizing and formatting your notebooks are included below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a timestamped log directory.\n",
    "#logdir = \"logs/train_data/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Creates a file writer for the log directory.\n",
    "#file_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# Using the file writer, log the reshaped image.\n",
    "#with file_writer.as_default():\n",
    "#  tf.summary.image(\"Training data\", test_img, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = Model(inputs=[self.model.inputs], outputs= [self.model.get_layer(self.layerName).output, self.model.output])\n",
    "\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "\n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "\n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "        colormap=cv2.COLORMAP_JET):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code and results matrix \n",
    "\n",
    "< code >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Algorithm and training code\n",
    "\n",
    "\n",
    "< code >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temporary training code\n",
    "used for heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False)\n",
    "\n",
    "newOutput=base_model.output\n",
    "newOutput=GlobalAveragePooling2D()(newOutput)\n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(512,activation='relu')(newOutput)\n",
    "\n",
    "preds=Dense(2,activation='softmax')(newOutput) #final layer with softmax activation\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "\n",
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)\n",
    "\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(stephens_path,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1,write_images=True)\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=5,\n",
    "                   callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image from disk (in OpenCV format) and then\n",
    "# resize the image to its target dimensions\n",
    "orig = cv2.imread(heatmap_files[4])\n",
    "resized = cv2.resize(orig, (224, 224))\n",
    "\n",
    "# load the input image from disk (in Keras/TensorFlow format) and\n",
    "# preprocess it\n",
    "image = load_img(heatmap_files[1], target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "# use the network to make predictions on the input imag and find\n",
    "# the class label index with the largest corresponding probability\n",
    "preds = model.predict(image)\n",
    "i = np.argmax(preds[0])\n",
    "\n",
    "# initialize our gradient class activation map and build the heatmap\n",
    "cam = GradCAM(model, i)\n",
    "heatmap = cam.compute_heatmap(image)\n",
    "\n",
    "# resize the resulting heatmap to the original input image dimensions\n",
    "# and then overlay heatmap on top of the image\n",
    "heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
    "(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n",
    "\n",
    "# draw the predicted label on the output image\n",
    "#cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)\n",
    "#cv2.putText(output, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "# display the original image and resulting heatmap and output image\n",
    "# to our screen\n",
    "output = np.vstack([orig, heatmap, output])\n",
    "output = imutils.resize(output, height=700)\n",
    "\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image from disk (in Keras/TensorFlow format) and\n",
    "# preprocess it\n",
    "#image = load_img(heatmap_files[44], target_size=(224, 224))\n",
    "#image = img_to_array(image)\n",
    "#image = np.expand_dims(image, axis=0)\n",
    "#image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "# use the network to make predictions on the input imag and find\n",
    "# the class label index with the largest corresponding probability\n",
    "#preds = model.predict(image)\n",
    "#i = np.argmax(preds[0])\n",
    "\n",
    "# initialize our gradient class activation map and build the heatmap\n",
    "#cam = GradCAM(model, i)\n",
    "#heatmap = cam.compute_heatmap(image)\n",
    "\n",
    "# resize the resulting heatmap to the original input image dimensions\n",
    "# and then overlay heatmap on top of the image\n",
    "#heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
    "#(heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n",
    "\n",
    "# draw the predicted label on the output image\n",
    "#cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)\n",
    "#cv2.putText(output, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "# display the original image and resulting heatmap and output image\n",
    "# to our screen\n",
    "#output = np.vstack([orig, heatmap, output])\n",
    "#output = imutils.resize(output, height=700)\n",
    "#plt.matshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class activation heatmap\n",
    "#heatmap = make_gradcam_heatmap(test_array, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "#plt.matshow(heatmap)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website\n",
    "\n",
    "\n",
    "< add photos of new website >\n",
    "\n",
    "add description of user experience, ideas, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The 10 rules\n",
    "These recommendations are based on [6]\n",
    "\n",
    "1. Tell a story for an audience: interleave explanatory text with code and results to create a computational narrative.\n",
    "\n",
    "2. Document the process, not just the results: so that others, and yourself later, will understand your reasoning and choices.\n",
    "\n",
    "3. Use cell divisions to make steps clear: have eaach cell perform one meaningful and documented step, avoid long cells.\n",
    "\n",
    "4. Modularize code: use and document functions for repeated operations to make the code more readable and save space.\n",
    "\n",
    "5. Record dependencies: use pip or Conda package manager, with environment.yml (Conda) or requirements.txt (pip). Please indicate package versions explicitly. Ideally, include a listing of dependencies at the bottom of the notebook. Make sure you test your notebook in an environment created from these dependencies, so that you don't add undocumened dependencies.\n",
    "\n",
    "6. Use version control: https://github.com/jupyter/nbdime is a diffing tool specifically designed for notebooks; you may find it useful.\n",
    "\n",
    "7. Build a pipeline: place key variable declarations at the top. Test with different parameters, clean up, and run all cells to prepare for potential non-interactive execution.\n",
    "\n",
    "8. Share and explain your data: if the original data are too big, inlude a sample. Make sure that the data is accessible. Include data description, and any processing done beforehand. Ideally, point to datasets that have been permanently managed and identified by DOIs.\n",
    "\n",
    "9. Design your notebooks to be read, run, and explored:  \n",
    "    9.1. Read: store it in a public code repository with a clear README file and a liberal open source license. Generate HTML/PDF versions of the final notebook.  \n",
    "    9.2. Run: use Binder to run it in the cloud, and/or create a Docker image.  \n",
    "    9.3. Explore: use interactive widgets (e.g., ipywidgets, or panel) \n",
    "    \n",
    "10. Advocate for open research: ask your colleagues or friends to try to run your notebooks.\n",
    "\n",
    "\n",
    "__Additional recommendations, beyond the 10 rules:__\n",
    "1. Do not include shell commands in cells (unless the notebook will be distributed in Docker container.)\n",
    "2. Keep cell output if other researchers will use the included data, or clear cell output if they will use their own data.\n",
    "3. Develop your code in a clean virtual environment, to avoid dependency conflicts and streamline packaging.\n",
    "\n",
    "To create a virtual environment, use\n",
    "```\n",
    "conda create -n <enviroment_name> python=3 jupyter -y\n",
    "```\n",
    "or\n",
    "```\n",
    "python3 -m venv /path/to/new/virtual/environment\n",
    "```\n",
    "and then activate it. See more about creating virtual environments in [9], [10].\n",
    "\n",
    "For additional useful guides see [1], [2], [3], [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then replace the default template.ipynb with this notebook or with the short version (just headings and brief explanations).\n",
    "\n",
    "Both the long and the short versions can be found at earthcube github.\n",
    "\n",
    "\n",
    "To make the changes effective after replacing the template and renaming it to template.ipynb, run in terminal:\n",
    "```\n",
    "jupyter nbextension install --py jupytemplate --sys-prefix\n",
    "jupyter nbextension enable jupytemplate/main --sys-prefix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Binder and Docker for your notebook repository\n",
    "\n",
    "Sharing just the Jupyter notebook .ipynb file is not sufficient for broad re-usability. We strongly recommend that Jupyter notebooks are submitted to EarthCube in Binders. The Binder service is a free service that allows anyone to run notebooks from their web browser without installing any software and resolving dependencies.\n",
    "\n",
    "Follow these steps:\n",
    " - Assemble the Jupyter notebooks , environment files, custom external libraries, datasets, and a README, in a single project directory (and sub-directories off of it, as needed). Use GitHub to organize your project as a GitHub repository.\n",
    "\n",
    "\n",
    " - Create Binder. Use http://mybinder.org to create a  URL for your notebook Binder (you will need to enter your GitHub repo URL). You can also add a Launch Binder button directly to your GitHub repo, by including the following in your README.md:\n",
    "\n",
    "```\n",
    "launch with myBinder\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/<path to your repo>)\n",
    "```\n",
    "\n",
    "Specific projects may have their own Binder implementations. In this case, include a project-specific Binder reference so that reviewers can execute your code.\n",
    "\n",
    " - Optionally, test your project with repo2docker. See instructions in [2]. This will create a docker image which can be made available in a Docker repository. \n",
    "\n",
    "\n",
    "Expect the reviewers to use a browser of their choice to run your code, and try to avoid special execution requirements that make it difficult for users to appreciate your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "List relevant references.\n",
    "\n",
    "1. Notebook sharing guidelines from reproducible-science-curriculum: https://reproducible-science-curriculum.github.io/publication-RR-Jupyter/\n",
    "2. Guide for developing shareable notebooks by Kevin Coakley, SDSC: https://github.com/kevincoakley/sharing-jupyter-notebooks/raw/master/Jupyter-Notebooks-Sharing-Recommendations.pdf\n",
    "3. Guide for sharing notebooks by Andrea Zonca, SDSC: https://zonca.dev/2020/09/how-to-share-jupyter-notebooks.html\n",
    "4. Jupyter Notebook Best Practices: https://towardsdatascience.com/jupyter-notebook-best-practices-f430a6ba8c69\n",
    "5. Introduction to Jupyter templates nbextension: https://towardsdatascience.com/stop-copy-pasting-notebooks-embrace-jupyter-templates-6bd7b6c00b94  \n",
    "    5.1. Table of Contents (Toc2) readthedocs: https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html  \n",
    "    5.2. Steps to install toc2: https://stackoverflow.com/questions/23435723/installing-ipython-notebook-table-of-contents\n",
    "6. Rule A, Birmingham A, Zuniga C, Altintas I, Huang SC, et al. (2019) Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks. PLOS Computational Biology 15(7): e1007007. https://doi.org/10.1371/journal.pcbi.1007007. Supplementary materials: example notebooks (https://github.com/jupyter-guide/ten-rules-jupyter) and tutorial (https://github.com/ISMB-ECCB-2019-Tutorial-AM4/reproducible-computational-workflows)\n",
    "7. Languages supported by Jupyter kernels: https://github.com/jupyter/jupyter/wiki/Jupyter-kernels\n",
    "8. EarthCube notebooks presented at EC Annual Meeting 2020: https://www.earthcube.org/notebooks\n",
    "9. Manage your Python Virtual Environment with Conda: https://towardsdatascience.com/manage-your-python-virtual-environment-with-conda-a0d2934d5195\n",
    "10. Venv - Creation of Virtual Environments: https://docs.python.org/3/library/venv.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
