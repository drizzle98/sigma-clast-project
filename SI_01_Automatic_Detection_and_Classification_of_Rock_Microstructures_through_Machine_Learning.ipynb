{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Detection and Classification of Rock Microstructures through Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "List authors, their current affiliations,  up-to-date contact information, and ORCID if available. Add as many author lines as you need. \n",
    "\n",
    "- Author1 = {\"name\": \"Stephen Iota\", \"affiliation\": \"USC\", \"email\": \"iota@usc.edu\", \"orcid\": \"\"}\n",
    "- Author2 = {\"name\": \"Junyi Liu\", \"affiliation\": \"USC\", \"email\": \"liujunyi@usc.edu\", \"orcid\": \"\"}\n",
    "- Author3 = {\"name\": \"Ming Lyu\", \"affiliation\": \"USC\", \"email\": \"minglyu@usc.edu\", \"orcid\": \"\"}\n",
    "- Author4 = {\"name\": \"Bolong Pan\", \"affiliation\": \"USC\", \"email\": \"bolongpa@usc.edu\", \"orcid\": \"\"}\n",
    "- Author5 = {\"name\": \"Xiaoyu Wang\", \"affiliation\": \"USC\", \"email\": \"xwang93@usc.edu\", \"orcid\": \"\"}\n",
    "- Author6 = {\"name\": \"Yolanda Gil\", \"affiliation\": \"USC\", \"email\": \"gil@isi.edu\", \"orcid\": \"\"}\n",
    "- Author7 = {\"name\": \"Wael AbdAlmageed\", \"affiliation\": \"USC\", \"email\": \"wamageed@isi.edu\", \"orcid\": \"\"}\n",
    "- Author8 = {\"name\": \"Gurman Gill\", \"affiliation\": \"Sonoma State\", \"email\": \"gillg@sonoma.edu\", \"orcid\": \"\"}\n",
    "- Author9 = {\"name\": \"Matty Mookerjee\", \"affiliation\": \"Sonoma State\", \"email\": \"matty.mookerjee@sonoma.edu\", \"orcid\": \"\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Template-Notebook-for-EarthCube---Long-Version\" data-toc-modified-id=\"Template-Notebook-for-EarthCube---Long-Version-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Template Notebook for EarthCube - Long Version</a></span><ul class=\"toc-item\"><li><span><a href=\"#Author(s)\" data-toc-modified-id=\"Author(s)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Author(s)</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Technical-contributions\" data-toc-modified-id=\"Technical-contributions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Technical contributions</a></span></li><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Methodology</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Work-In-Progress---improvements\" data-toc-modified-id=\"Work-In-Progress---improvements-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Work In Progress - improvements</a></span></li><li><span><a href=\"#Suggested-next-steps\" data-toc-modified-id=\"Suggested-next-steps-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Suggested next steps</a></span></li><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Acknowledgements</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-import\" data-toc-modified-id=\"Library-import-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Library import</a></span></li><li><span><a href=\"#Local-library-import\" data-toc-modified-id=\"Local-library-import-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Local library import</a></span></li></ul></li><li><span><a href=\"#Parameter-definitions\" data-toc-modified-id=\"Parameter-definitions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parameter definitions</a></span></li><li><span><a href=\"#Data-import\" data-toc-modified-id=\"Data-import-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data import</a></span></li><li><span><a href=\"#Data-processing-and-analysis\" data-toc-modified-id=\"Data-processing-and-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data processing and analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-10-rules\" data-toc-modified-id=\"The-10-rules-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>The 10 rules</a></span></li><li><span><a href=\"#Using-notebook-template\" data-toc-modified-id=\"Using-notebook-template-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Using notebook template</a></span></li><li><span><a href=\"#Adding-table-of-contents\" data-toc-modified-id=\"Adding-table-of-contents-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Adding table of contents</a></span></li><li><span><a href=\"#Creating-Binder-and-Docker-for-your-notebook-repository\" data-toc-modified-id=\"Creating-Binder-and-Docker-for-your-notebook-repository-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Creating Binder and Docker for your notebook repository</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Geologists need help classifying microscope rock images of sigma clasts; a type of mantled porphyroclasts widely used as kinematic indicators in rocks. \n",
    "Knowledge about the sheer sense of sigma clast during formation (either CCW or CW sheering) gives insights into rock formation history.  \n",
    "\n",
    "This work reports on early investigation of machine learning techniques for automatic detection and classification of sigma clasts and their rotation from photomicrographs. \n",
    "Convolutional Neural Networks (CNNs) are used to extract and leverage defining features of sigma clasts, such as shape, color, texture, and tail direction to improve accuracy. \n",
    "\n",
    "We leverage existing models that are pre-trained on very large collections of images, and use transfer learning techniques to apply them to microstructure images.  \n",
    "\n",
    "We used YOLOv3 to identify different sigma clasts in a given image. We also experimented with other large pre-trained models such as ResNet50, VGG19, Inceptionv3 with two additional layers trained specifically on our dataset. \n",
    "\n",
    "In order to facilitate exploration of different models with different settings, we are developing a computational experimentation environment to visualize different CNN network layers, classification heatmaps, and comparative metrics. \n",
    "\n",
    "Finally, since models perform better when more data are available, we are developing a web application to collect additional data from geoscientists and incentivize their participation in open science. \n",
    "\n",
    "The website allows researchers to upload images of rock microstructures, showing them the classification of the images based on the best models available, and allows them to correct any errors which can be used to improve the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical contributions\n",
    "Summarize the central contributions of the notebook (libraries created, scientific analysis demonstrated, etc.), as a bulleted list, with one or more bullets.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- demonstration of scientific analysis leveraging existing API (explain)\n",
    "- development of underlying API that is exposed in the notebook (state which components are developed by the author, and point to documentation)\n",
    "- developmnent of local libraries imported in the notebook (they should be subbmitted along with the notebook and documented in the same way)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "Quickly describe assumptions and processing steps. Include URLs as necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Describe and comment on the most important results. Include images and URLs as necessary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding\n",
    "- No funding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "keywords=[\"Sigma clast\", \"classification\", \"machine learning\", \"AI for geosciences\", \"CNNs for image classification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "S. Iota et. al, Automatic Detection and Classification of Rock Microstructures through Machine Learning, EarthCube Annual Meeting, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work In Progress - improvements\n",
    "Use this section only if the notebook is not final.\n",
    "\n",
    "Notable TODOs:\n",
    "- Data augmentation;\n",
    "- Integrate image recognition models with user website;\n",
    "- todo 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested next steps\n",
    "State suggested next steps, based on results obtained in this notebook. This section is optional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements \n",
    "\n",
    "Include any relevant acknowledgements, apart from funding (which was in section 1.6)\n",
    "\n",
    "This notebook template extends the original notebook template provided with the jupytemplate extension [5]. It is a result of collaboration between the TAC Working Group and the EarthCube Office. \n",
    "\n",
    "The template is licensed under a <a href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n",
    "Import all the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add any additional imports you needed down below, try to group them by their purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std library \n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML models\n",
    "import keras\n",
    "import cv2\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50,VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local library import\n",
    "Import all the required local libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## None needed so far, but add them here if you need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions\n",
    "Set all relevant parameters for the notebook. By convention, parameters are uppercase, while all the other variables follow Python's guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n",
    "Retrieve all the required data for the analysis. Include brief data descriptions, and DOIs where possible.\n",
    "\n",
    "Please import data in one of two ways:\n",
    "1. Reference data that is available online and continuously managed (preferred). \n",
    "2. If the data is relatively small, please include the files along with the notebook. If you deal with large data, include a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we should import all the data files,\n",
    "## we need to make all data available locally (inside a Binder),\n",
    "## so maybe we should only use a small subset of the data for demonstration purposes\n",
    "## then give a pointer to where the rest of the data can be found on line\n",
    "## (maybe thru a gdrive link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n",
    "The core of the notebook is here. Split this section into subsections as required, and explain processing and analysis steps.\n",
    "\n",
    "Selected best practices for organizing and formatting your notebooks are included below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-752545de8596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayerName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# store the model, the class index used to measure the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# activation map, and the layer to be used when visualizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# the class activation map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-752545de8596>\u001b[0m in \u001b[0;36mGradCAM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     def overlay_heatmap(self, heatmap, image, alpha=0.5,\n\u001b[0;32m---> 81\u001b[0;31m         colormap=cv2.COLORMAP_JET):\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;31m# apply the supplied color map to the heatmap and then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# overlay the heatmap on the input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = Model(inputs=[self.model.inputs], outputs= [self.model.get_layer(self.layerName).output, self.model.output])\n",
    "\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "\n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "\n",
    "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
    "        colormap=cv2.COLORMAP_JET):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        return (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    print(grads.shape)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))#, 2))\n",
    "    print(pooled_grads.shape)\n",
    "\n",
    "    # multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=VGG19(weights='imagenet',include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(folder_path,\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz -- heatmaps and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(test_array, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The 10 rules\n",
    "These recommendations are based on [6]\n",
    "\n",
    "1. Tell a story for an audience: interleave explanatory text with code and results to create a computational narrative.\n",
    "\n",
    "2. Document the process, not just the results: so that others, and yourself later, will understand your reasoning and choices.\n",
    "\n",
    "3. Use cell divisions to make steps clear: have eaach cell perform one meaningful and documented step, avoid long cells.\n",
    "\n",
    "4. Modularize code: use and document functions for repeated operations to make the code more readable and save space.\n",
    "\n",
    "5. Record dependencies: use pip or Conda package manager, with environment.yml (Conda) or requirements.txt (pip). Please indicate package versions explicitly. Ideally, include a listing of dependencies at the bottom of the notebook. Make sure you test your notebook in an environment created from these dependencies, so that you don't add undocumened dependencies.\n",
    "\n",
    "6. Use version control: https://github.com/jupyter/nbdime is a diffing tool specifically designed for notebooks; you may find it useful.\n",
    "\n",
    "7. Build a pipeline: place key variable declarations at the top. Test with different parameters, clean up, and run all cells to prepare for potential non-interactive execution.\n",
    "\n",
    "8. Share and explain your data: if the original data are too big, inlude a sample. Make sure that the data is accessible. Include data description, and any processing done beforehand. Ideally, point to datasets that have been permanently managed and identified by DOIs.\n",
    "\n",
    "9. Design your notebooks to be read, run, and explored:  \n",
    "    9.1. Read: store it in a public code repository with a clear README file and a liberal open source license. Generate HTML/PDF versions of the final notebook.  \n",
    "    9.2. Run: use Binder to run it in the cloud, and/or create a Docker image.  \n",
    "    9.3. Explore: use interactive widgets (e.g., ipywidgets, or panel) \n",
    "    \n",
    "10. Advocate for open research: ask your colleagues or friends to try to run your notebooks.\n",
    "\n",
    "\n",
    "__Additional recommendations, beyond the 10 rules:__\n",
    "1. Do not include shell commands in cells (unless the notebook will be distributed in Docker container.)\n",
    "2. Keep cell output if other researchers will use the included data, or clear cell output if they will use their own data.\n",
    "3. Develop your code in a clean virtual environment, to avoid dependency conflicts and streamline packaging.\n",
    "\n",
    "To create a virtual environment, use\n",
    "```\n",
    "conda create -n <enviroment_name> python=3 jupyter -y\n",
    "```\n",
    "or\n",
    "```\n",
    "python3 -m venv /path/to/new/virtual/environment\n",
    "```\n",
    "and then activate it. See more about creating virtual environments in [9], [10].\n",
    "\n",
    "For additional useful guides see [1], [2], [3], [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then replace the default template.ipynb with this notebook or with the short version (just headings and brief explanations).\n",
    "\n",
    "Both the long and the short versions can be found at earthcube github.\n",
    "\n",
    "\n",
    "To make the changes effective after replacing the template and renaming it to template.ipynb, run in terminal:\n",
    "```\n",
    "jupyter nbextension install --py jupytemplate --sys-prefix\n",
    "jupyter nbextension enable jupytemplate/main --sys-prefix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Binder and Docker for your notebook repository\n",
    "\n",
    "Sharing just the Jupyter notebook .ipynb file is not sufficient for broad re-usability. We strongly recommend that Jupyter notebooks are submitted to EarthCube in Binders. The Binder service is a free service that allows anyone to run notebooks from their web browser without installing any software and resolving dependencies.\n",
    "\n",
    "Follow these steps:\n",
    " - Assemble the Jupyter notebooks , environment files, custom external libraries, datasets, and a README, in a single project directory (and sub-directories off of it, as needed). Use GitHub to organize your project as a GitHub repository.\n",
    "\n",
    "\n",
    " - Create Binder. Use http://mybinder.org to create a  URL for your notebook Binder (you will need to enter your GitHub repo URL). You can also add a Launch Binder button directly to your GitHub repo, by including the following in your README.md:\n",
    "\n",
    "```\n",
    "launch with myBinder\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/<path to your repo>)\n",
    "```\n",
    "\n",
    "Specific projects may have their own Binder implementations. In this case, include a project-specific Binder reference so that reviewers can execute your code.\n",
    "\n",
    " - Optionally, test your project with repo2docker. See instructions in [2]. This will create a docker image which can be made available in a Docker repository. \n",
    "\n",
    "\n",
    "Expect the reviewers to use a browser of their choice to run your code, and try to avoid special execution requirements that make it difficult for users to appreciate your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "List relevant references.\n",
    "\n",
    "1. Notebook sharing guidelines from reproducible-science-curriculum: https://reproducible-science-curriculum.github.io/publication-RR-Jupyter/\n",
    "2. Guide for developing shareable notebooks by Kevin Coakley, SDSC: https://github.com/kevincoakley/sharing-jupyter-notebooks/raw/master/Jupyter-Notebooks-Sharing-Recommendations.pdf\n",
    "3. Guide for sharing notebooks by Andrea Zonca, SDSC: https://zonca.dev/2020/09/how-to-share-jupyter-notebooks.html\n",
    "4. Jupyter Notebook Best Practices: https://towardsdatascience.com/jupyter-notebook-best-practices-f430a6ba8c69\n",
    "5. Introduction to Jupyter templates nbextension: https://towardsdatascience.com/stop-copy-pasting-notebooks-embrace-jupyter-templates-6bd7b6c00b94  \n",
    "    5.1. Table of Contents (Toc2) readthedocs: https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html  \n",
    "    5.2. Steps to install toc2: https://stackoverflow.com/questions/23435723/installing-ipython-notebook-table-of-contents\n",
    "6. Rule A, Birmingham A, Zuniga C, Altintas I, Huang SC, et al. (2019) Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks. PLOS Computational Biology 15(7): e1007007. https://doi.org/10.1371/journal.pcbi.1007007. Supplementary materials: example notebooks (https://github.com/jupyter-guide/ten-rules-jupyter) and tutorial (https://github.com/ISMB-ECCB-2019-Tutorial-AM4/reproducible-computational-workflows)\n",
    "7. Languages supported by Jupyter kernels: https://github.com/jupyter/jupyter/wiki/Jupyter-kernels\n",
    "8. EarthCube notebooks presented at EC Annual Meeting 2020: https://www.earthcube.org/notebooks\n",
    "9. Manage your Python Virtual Environment with Conda: https://towardsdatascience.com/manage-your-python-virtual-environment-with-conda-a0d2934d5195\n",
    "10. Venv - Creation of Virtual Environments: https://docs.python.org/3/library/venv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
